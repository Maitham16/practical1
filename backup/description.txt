hey, I am working on a study of distributed data streaming process. Now, it is the time for practical part. I want to simulate the process, for that I implemented cars scripts, 4 nodes, and I have implemented a central server that will receive the data and trained models from nodes for further training and send the updates back. all of these should be done on my laptop, for that, I implemented everything using jupyter notebook on vs code, debian os, kafka, and apache spark. the scenario simulates the real time data processing and prediction for the need of EV to charge. I implemented the scenario that the car travels along a road long 600 km and passes through 4 nodes, also, there are 3 charging points along the road. I want to improve that the process of staleness data and when the model drop down the real time data can makes the model works better. I pasted below one code of the 26 cars as only the variables difference, one node script of the 4 nodes as all are the same, and one model of 4 models for each node one ML model but all the same script just the data difference:

car script:
  from kafka import KafkaProducer
import json
import time
import random
import datetim
# Constants
BASE_SPEED = 90  # km/h
CONSUMPTION_MODIFIER = 0.002  # arbitrary factor to simulate increased consumption at higher speeds
CHARGING_STATIONS = [150, 270, 460]  # Charging station locations
BATTERY_LIFE_DEGRADATION_RATE = 0.001 # % per km
DEGRADATION_FACTOR = 0.5    # arbitrary factor to simulate increased consumption at lower battery life
WEATHER_CONDITIONS = ['sunny', 'rainy', 'snowy']
TRAFFIC_CONDITIONS = ['light', 'moderate', 'heavy']
ROAD_GRADIENT = ['flat', 'uphill', 'downhill']
EMERGENCY_SITUATIONS = ['none', 'accident_ahead', 'road_closure']
SIMULATION_SPEED_FACTOR = 60  # 1 = real time, 60 = 1 hour per second, 3600 = 1 day per second 
real_start_time = datetime.datetime.now()
simulated_start_time = datetime.datetime.now()
def create_producer():
    return KafkaProducer(
        bootstrap_servers='localhost:9092',
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )
def get_simulated_timestamp(real_start_time, simulated_start_time, speed_factor):
    real_elapsed = datetime.datetime.now() - real_start_time
    simulated_elapsed = real_elapsed * speed_factor
    return simulated_start_time + simulated_elapsed
def calculate_distance_to_next_station(location):
    next_station = next((station for station in CHARGING_STATIONS if station > location), None)
    if next_station:
        return next_station - location
    return float('inf')  # No next station found
def update_car_data_while_charging(data):
    data['charge'] += 30  # 30% charge per 30 minutes
    if data['charge'] > 100:   # Ensure charging does not exceed 100%
        data['charge'] = 100
    if random.random() < 0.5:  # 50% chance to stop charging each time step
        data['charging'] = False
def update_car_data_based_on_weather(data, modified_consumption):
    weather = random.choice(WEATHER_CONDITIONS)
    if weather == 'rainy':
        modified_consumption *= 1.1
    elif weather == 'snowy':
        modified_consumption *= 1.2
    return modified_consumption
def update_car_data_based_on_traffic(data, modified_consumption):
    traffic = random.choice(TRAFFIC_CONDITIONS)
    if traffic == 'moderate':
        data['current_speed'] *= 0.8
        modified_consumption *= 1.1
    elif traffic == 'heavy':
        data['current_speed'] *= 0.6
        modified_consumption *= 1.2
    return modified_consumption
def update_car_data_based_on_gradient(data, modified_consumption):
    gradient = random.choice(ROAD_GRADIENT)
    if gradient == 'uphill':
        modified_consumption *= 1.2
    elif gradient == 'downhill':
        modified_consumption *= 0.8
    return modified_consumption
def handle_emergency_situations(data):
    weighted_emergency_list = ['none'] * 95 + ['accident_ahead', 'road_closure']
    emergency = random.choice(weighted_emergency_list)
    if emergency == 'accident_ahead':
        data['car_status'] = 'stopped'
        data['current_speed'] = 0
        data['emergency_duration'] = random.randint(5, 15)  # Random duration between 5 to 15 minutes
    elif emergency == 'road_closure':
        data['car_status'] = 'rerouting'
        data['current_speed'] *= 0.5
        data['emergency_duration'] = random.randint(5, 15)  # Random duration between 5 to 15 minutes
def update_car_data_while_moving(data, distance_step):
    # Updating location, distance covered, and current speed
    data['location'] += distance_step # km
    data['distance_covered'] += distance_step # km
    speed_factor = data['current_speed'] / BASE_SPEED 
    adjusted_consumption = data['consumption'] * speed_factor + (CONSUMPTION_MODIFIER * (speed_factor - 1))
    modified_consumption = adjusted_consumption * (1 + ((100 - data['battery_life']) / 100) * DEGRADATION_FACTOR)
    data['charge'] -= modified_consumption * distance_step / data['battery_capacity'] * 100
    data['battery_life'] -= BATTERY_LIFE_DEGRADATION_RATE
    # Calculate distance to next charging point and add to data
    data['distance_to_charging_point'] = calculate_distance_to_next_station(data['location'])
    modified_consumption = update_car_data_based_on_weather(data, modified_consumption)
    modified_consumption = update_car_data_based_on_traffic(data, modified_consumption)
    modified_consumption = update_car_data_based_on_gradient(data, modified_consumption)
    handle_emergency_situations(data)
    weather = random.choice(WEATHER_CONDITIONS)
    traffic = random.choice(TRAFFIC_CONDITIONS)
    gradient = random.choice(ROAD_GRADIENT)
    emergency = random.choice(EMERGENCY_SITUATIONS)
    # Check if the current location is approaching a charging station and if charge is low
    if data['charge'] <= 30:
        next_station = next((station for station in CHARGING_STATIONS if station > data['location']), None)
        if next_station:
            distance_to_next_station = next_station - data['location']
            charge_required_to_reach_station = distance_to_next_station / data['battery_capacity'] * data['consumption'] * 100
            if data['charge'] >= charge_required_to_reach_station:
                data['location'] = next_station
                data['charging'] = True
                data['car_status'] = "Charging"
            else:
                distance_possible = data['charge'] / data['consumption'] * data['battery_capacity'] / 100
                data['location'] += distance_possible
                data['charge'] = 0
                data['charging'] = False
                data['car_status'] = "Moving"
    # Randomly increase or decrease the current speed
    speed_variation = 10 # km/h
    min_speed = 70
    max_speed = 180
    current_speed = int(data['current_speed'])
    if current_speed <= min_speed:
        new_speed = random.randint(min_speed, min(max_speed, min_speed + speed_variation))
    elif current_speed >= max_speed:
        new_speed = random.randint(max(min_speed, max_speed - speed_variation), max_speed)
    else:
        new_speed = random.randint(max(min_speed, current_speed - speed_variation),
                                    min(max_speed, current_speed + speed_variation))
    data['current_speed'] = new_speed
    # Update the node based on the current location
    if data['location'] < 150:
        data['node'] = 'Node 1'
    elif data['location'] < 280:
        data['node'] = 'Node 2'
    elif data['location'] < 420:
        data['node'] = 'Node 3'
    elif data['location'] < 600:
        data['node'] = 'Node 4'
    else:
        print("Car has reached its destination.")
     # After the existing code, add logic to handle resuming speed or rerouting after emergency
    if data['emergency_duration'] > 0:
        time.sleep(data['emergency_duration'] * 60 / SIMULATION_SPEED_FACTOR)
        data['emergency_duration'] = 0
        if emergency == 'accident_ahead':
            data['car_status'] = 'moving'
            data['current_speed'] = random.randint(50, 100)
        elif emergency == 'road_closure':
            data['car_status'] = 'moving'
            data['current_speed'] = random.randint(50, 100)
    return modified_consumption, weather, traffic, gradient, emergency, data['emergency_duration']
def main():
    producer = create_producer()
    # Initial data
    data = {
        'car_id': 'car1',
        'model': 'Model D',
        'current_speed': 50,
        'battery_capacity': 50,
        'charge': 99,
        'consumption': 0.13,
        'location': 110,
        'node': 'Node 1',
        'car_status': "moving",
        'distance_covered': 0,
        'battery_life': 98,
        'distance_to_charging_point': calculate_distance_to_next_station(110),
        'charging': False,
        'emergency_duration': 0
    }
    time_step = 1 # minutes per time step
    distance_step = data['current_speed'] * (time_step/60) # km per time step
    counter = 0 # time step counter
    while data['charge'] > 0 and data['distance_covered'] < 600:
        counter += 1
        if data['charging']:
            update_car_data_while_charging(data)
        else:
            modified_consumption, weather, traffic, gradient, emergency, emergency_duration = update_car_data_while_moving(data, distance_step)
        # Add new fields to the data dictionary
        data['weather'] = weather
        data['traffic'] = traffic
        data['road_gradient'] = gradient
        data['emergency'] = emergency
        data['emergency_duration'] = emergency_duration
        # Recalculate the distance_step based on current speed
        distance_step = data['current_speed'] * (time_step/60)
        # Ensure charge does not go negative
        if data['charge'] < 0:
            data['charge'] = 0
            print("Car has run out of charge and has stopped.")
            break
        data['timestamp'] = get_simulated_timestamp(real_start_time, simulated_start_time, SIMULATION_SPEED_FACTOR).strftime('%Y-%m-%d %H:%M:%S')
        # Print current status
        print(f"Time step: {counter*time_step} minutes")
        print(f"Current speed: {data['current_speed']} km/h")
        print(f"Distance covered in this step: {distance_step} km")
        print(f"Charge consumed in this step: {modified_consumption * distance_step} %")
        print(f"Remaining charge: {data['charge']} %")
        print(f"Total distance covered: {data['distance_covered']} km")
        print(f"Battery life: {data['battery_life']} %")
        print(f"Weather: {weather}")
        print(f"Traffic: {traffic}")
        print(f"Road Gradient: {gradient}")
        print(f"Emergency Situation: {emergency}")
        print(f"Emergency Duration: {emergency_duration} minutes")
        print("--------------------------")
        # Send data to Kafka
        producer.send('electric_car_data', data)
        if data['node'] == 'Node 1':
            producer.send('node1_data', data)
        elif data['node'] == 'Node 2':
            producer.send('node2_data', data)
        elif data['node'] == 'Node 3':
            producer.send('node3_data', data)
        else:
            producer.send('node4_data', data)
        time.sleep(time_step * 60 / SIMULATION_SPEED_FACTOR)
if __name__ == "__main__":
    main()

node script:

import csv
import json
import joblib
import numpy as np
import tensorflow as tf
from kafka import KafkaConsumer
# Load Random Forest model
rf_model = joblib.load('./random_forest_model_node_1.pkl')
# Load Neural Network model
nn_model = tf.keras.models.load_model('./neural_network_model_node_1.h5')
# Load scaler object
scaler = joblib.load('./scaler_node_1.pkl')
def create_csv_writer(filename, columns):
    try:
        file = open(filename, 'w', newline='')
        writer = csv.writer(file)
        writer.writerow(columns)
        return file, writer
    except Exception as e:
        print(f"Error creating CSV writer: {e}")
        raise
def write_data_to_csv(writer, data, columns):
    try:
        writer.writerow([data[col] for col in columns])
    except Exception as e:
        print(f"Error writing data to CSV: {e}")
        raise
def process_data(data):
    data['needs_charge'] = 1 if float(data['charge']) <= 50 else 0
    features = [
        float(data["current_speed"]),
        float(data["battery_capacity"]),
        float(data["charge"]),
        float(data["consumption"]),
        float(data["distance_covered"]),
        float(data["battery_life"]),
        float(data["distance_to_charging_point"]),
        float(data["emergency_duration"])
    ]
    return features
def predict_and_print(data):
    try:
        data['needs_charge'] = 1 if float(data['charge']) <= 50 else 0
        features = process_data(data)
        prediction_rf = predict_need_charge(rf_model, scaler, features)
        prediction_nn = predict_need_charge(nn_model, scaler, features)
        print(f"Random Forest Prediction: {prediction_rf}")
        print(f"Neural Network Prediction: {prediction_nn}")
    except Exception as e:
        print(f"Error predicting data: {e}")
        raise
def predict_need_charge(model, scaler, features):
    # scale the features
    features_scaled = scaler.transform(np.array(features).reshape(1, -1))
    # make prediction
    prediction = model.predict(features_scaled)
    return int(prediction.round())
def start_consumer(topic_name, csv_filename):
    file = None
    try:
        consumer = KafkaConsumer(
            topic_name,
            bootstrap_servers='localhost:9092',
            value_deserializer=lambda v: json.loads(v.decode('utf-8')),
            auto_offset_reset='earliest'
        )
        columns = [
            "timestamp", "car_id", "model", "current_speed", "battery_capacity",
            "charge", "consumption", "location", "node", "car_status",
            "distance_covered", "battery_life", "distance_to_charging_point",
            "weather", "traffic", "road_gradient", "emergency", "emergency_duration"
        ]
        file, writer = create_csv_writer(csv_filename, columns)
        try:
            for msg in consumer:
                data = msg.value
                print(f"Received data from {topic_name}: {data}")
                write_data_to_csv(writer, data, columns)
                file.flush()
                predict_and_print(data)
        except KeyboardInterrupt:
            pass
    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        if file:
            file.close()
if __name__ == "__main__":
    start_consumer('node1_data', 'node1_data.csv')








central server script:
import numpy as np
import torch
import torch.nn as nn
import socket
import pickle
import argparse
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Central server for federated learning.')
parser.add_argument('--num_nodes', type=int, default=2, help='Number of local nodes')
parser.add_argument('--num_rounds', type=int, default=10, help='Number of communication rounds')
args = parser.parse_args()

NUM_LOCAL_NODES = args.num_nodes
NUM_ROUNDS = args.num_rounds

# Initialize global model
def initialize_model():
    logging.info('Initializing global model')
    # Define your model architecture
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = nn.Linear(28 * 28, 200)
            self.fc2 = nn.Linear(200, 10)

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = self.fc2(x)
            return x
    
    # Initialize model
    global_model = Net()
    return global_model

# Receive model updates from local nodes
def receive_local_updates():
    logging.info('Receiving local updates')
    local_updates = []

    # Set up the server socket
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('localhost', 12345))
        s.listen()

        # Receive updates from all local nodes
        for _ in range(NUM_LOCAL_NODES):
            conn, addr = s.accept()
            with conn:
                print('Connected by', addr)
                data = b''
                while True:
                    packet = conn.recv(4096)
                    if not packet:
                        break
                    data += packet
                local_update = pickle.loads(data)
                local_updates.append(local_update)
    
    return local_updates

# Aggregate local updates
def aggregate_updates(local_updates):
    logging.info('Aggregating local updates')
    # Compute the average of all local updates
    aggregated_updates = np.mean(local_updates, axis=0)
    return aggregated_updates

# Update global model
def update_global_model(global_model, aggregated_updates):
    logging.info('Updating global model')
    for param, update in zip(global_model.parameters(), aggregated_updates):
        param.data = torch.Tensor(update)
    return global_model

# Broadcast global model
def send_global_model(global_model):
    logging.info('Broadcasting global model')
    # Serialize the global model
    serialized_model = pickle.dumps(global_model)

    # Set up the server socket
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('localhost', 12346))
        s.listen()

        # Send the global model to all local nodes
        for _ in range(NUM_LOCAL_NODES):
            conn, addr = s.accept()
            with conn:
                print('Connected by', addr)
                conn.sendall(serialized_model)

# Main function
def main():
    # Initialize global model
    global_model = initialize_model()

    # Main loop
    for round in range(NUM_ROUNDS):
        logging.info(f'Starting communication round {round+1}/{NUM_ROUNDS}')
        
        # Receive model updates from local nodes
        local_updates = receive_local_updates()
        
        # Aggregate local updates
        aggregated_updates = aggregate_updates(local_updates)
        
        # Update global model
        global_model = update_global_model(global_model, aggregated_updates)
        
        # Broadcast global model
        send_global_model(global_model)

if __name__ == "__main__":
    main()

model script:

# imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import StandardScaler
import joblib
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
# Load the data
data = pd.read_csv('../dataset/node1_data.csv')
data['needs_charge'] = np.where(data['charge'] <= 50, 1, 0)
# Data Cleaning
data = pd.get_dummies(data, columns=['weather', 'traffic', 'road_gradient', 'emergency', 'car_status'])
# Check the column names
print(data.columns)
## Remove duplicates
data = data.drop_duplicates()
# Drop other non-numeric columns if any
data = data.select_dtypes(include=[float, int])
## Handle outliers
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
for column in data.columns:
    median = data[column].median()
    data.loc[((data[column] < (Q1[column] - 1.5 * IQR[column])) | (data[column] > (Q3[column] + 1.5 * IQR[column]))), column] = median
# Feature Selection
features = ['current_speed', 'battery_capacity', 'charge', 'consumption', 'distance_covered', 'battery_life', 'distance_to_charging_point']
features += [col for col in data if 'weather_' in col]
features += [col for col in data if 'traffic_' in col]
features += [col for col in data if 'road_gradient_' in col]
features += [col for col in data if 'emergency_' in col]
features += [col for col in data if 'car_status_' in col]
X = data[features]
y = data['needs_charge']
print(X.head())
# RFE
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
# Initialize the model with higher max_iter
model = LogisticRegression(max_iter=1000)
# Initialize RFE
rfe = RFE(model, n_features_to_select=10)  # change the number of features to select as needed
# Fit RFE
fit = rfe.fit(X, y)
# Get the selected features
selected_features = X.columns[fit.support_]
print("Selected Features: ", selected_features)
print(data['charge'].describe())
print(data['needs_charge'].value_counts())
print(data['needs_charge'].value_counts())
# Data Splitting using selected features
X_selected = X[selected_features]
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)
# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Data Visualization
# Distribution of the target variable
plt.figure(figsize=(12,8))
sns.histplot(data['needs_charge'], kde=True)
plt.title('Distribution of Needs Charging Rate')
plt.xlabel('Needs Charging Rate')
plt.ylabel('Count')
plt.show()
# plot the distribution 
plt.figure(figsize=(12,8))
sns.histplot(data['charge'], kde=True)
plt.title('Distribution of Charge')
plt.xlabel('Charge')
plt.ylabel('Count')
plt.show()
## Correlation Analysis
correlation_matrix = data.corr()
plt.figure(figsize=(30,15))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()
# Visualizing the distribution of the features
data[features].hist(figsize=(20, 20), bins=50)
plt.show()
# Pairplot of the features and target variable
sns.pairplot(data[features + ['needs_charge']], hue='needs_charge')
plt.show()
# Boxplots for each feature
plt.figure(figsize=(20, 10))
sns.boxplot(data=data[features])
plt.title('Boxplot of Features')
plt.xticks(rotation=90)
plt.show()
# Scatter plots
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='current_speed', y='battery_life', hue='needs_charge')
plt.title('Scatter Plot of Current Speed vs Battery Life')
plt.show()
# Violin plots
plt.figure(figsize=(10, 6))
sns.violinplot(x=data['needs_charge'], y=data['current_speed'])
plt.title('Violin Plot of Current Speed vs Needs Charge')
plt.show()
# cluster plot
from sklearn.cluster import KMeans
# Fit the KMeans algorithm to the data
kmeans = KMeans(n_clusters=3)
data['cluster'] = kmeans.fit_predict(data[features])
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='current_speed', y='battery_life', hue='cluster')
plt.title('Cluster Plot of Current Speed vs Battery Life')
plt.show()
# Dimensionality reduction plot
from sklearn.decomposition import PCA
# Apply PCA to the data
pca = PCA(n_components=2)
pca_result = pca.fit_transform(data[features])
data['pca_1'] = pca_result[:, 0]
data['pca_2'] = pca_result[:, 1]
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='pca_1', y='pca_2', hue='needs_charge')
plt.title('PCA Plot')
plt.show()
# Scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='location', y='current_speed', hue='needs_charge')
plt.title('Current Speed vs Location')
plt.xlabel('Location (km)')
plt.ylabel('Current Speed (km/h)')
plt.show()
plt.figure(figsize=(15, 7))
plt.plot(data['location'], data['current_speed'], label='Current Speed')
plt.plot(data['location'], data['battery_life'], label='Battery Life')
plt.title('Current Speed and Battery Life along the Route')
plt.xlabel('Location (km)')
plt.ylabel('Value')
plt.legend()
plt.show()
# Random Forest Classifier
# Build the model
from sklearn.ensemble import RandomForestClassifier
# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300, 400],
    'max_depth': [10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
# Create a based model
rf = RandomForestClassifier(random_state=42)
# Instantiate the grid search model
from sklearn.model_selection import GridSearchCV
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 3, n_jobs = -1, verbose = 2)
# Fit the grid search to the data
grid_search.fit(X_train, y_train)
# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters: ", best_params)
# Train the model with the best parameters
model = grid_search.best_estimator_
# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, y_pred))
# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, y_pred))
from sklearn.model_selection import cross_val_score
# Use cross_val_score function
# Cross-Validation
scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')
print('Cross-Validation Accuracy Scores', scores)
print('Average Cross-Validation Accuracy', scores.mean())
# Initialize the constructor
model = Sequential()
# Add an input layer 
model.add(Dense(12, activation='relu', input_shape=(len(X.columns),)))
# Add one hidden layer 
model.add(Dense(8, activation='relu'))
# Add an output layer 
model.add(Dense(1, activation='sigmoid'))
# Compile the model
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model.summary()
# Define the callbacks
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

callbacks = [EarlyStopping(patience=10, restore_best_weights=True),
             ModelCheckpoint(filepath='best_model.h5', save_best_only=True)]

# Fit the model
model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1, 
          validation_data=(X_test, y_test), callbacks=callbacks)
# Evaluate the model
score = model.evaluate(X_test, y_test, verbose=1)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
def predict_need_charge(model, scaler, features):
    # print feature names and their values 
    # scale the features
    features_scaled = scaler.transform(np.array(features).reshape(1, -1))
    # make prediction
    prediction = model.predict(features_scaled)
    return int(prediction.round())
features = [
    90,  # current_speed
    35,  # battery_capacity
    49,  # charge
    0.1,  # consumption
    100,  # distance_covered
    100,  # battery_life
    5,   # distance_to_charging_point
    0    # emergency_duration
]
prediction = predict_need_charge(model, scaler, features)
if prediction == 1:
    print("The car needs to charge in the next station.")
else:
    print("The car does not need to charge in the next station.")
# save Random Forest model
joblib.dump(model, 'random_forest_model_node_1.pkl')
# save Neural Network model
model.save('neural_network_model_node_1.h5')
# save scaler object
joblib.dump(scaler, 'scaler_node_1.pkl')
# load Random Forest model
model = joblib.load('random_forest_model_node_1.pkl')
# load Neural Network model
model = tf.keras.models.load_model('neural_network_model_node_1.h5')
# load scaler object
scaler = joblib.load('scaler_node_1.pkl')
def predict_need_charge(model, scaler, features):
    # scale the features
    features_scaled = scaler.transform(np.array(features).reshape(1, -1))
    # make prediction
    prediction = model.predict(features_scaled)
    return int(prediction.round())
features = [
    80,  # current_speed
    35,  # battery_capacity
    45,  # charge
    0.1,  # consumption
    100,  # distance_covered
    100,  # battery_life
    5,   # distance_to_charging_point
    0    # emergency_duration
]
prediction = predict_need_charge(model, scaler, features)
if prediction == 1:
    print("The car needs to charge in the next station.")
else:
    print("The car does not need to charge in the next station.")




sample of data:

timestamp,car_id,model,current_speed,battery_capacity,charge,consumption,location,node,car_status,distance_covered,battery_life,distance_to_charging_point,weather,traffic,road_gradient,emergency,emergency_duration
2023-08-30 02:17:36,car1,Model D,21,50,68.87992222222222,0.13,110.83333333333333,Node 1,moving,0.8333333333333334,97.999,39.16666666666667,snowy,moderate,uphill,none,0
2023-08-30 02:17:41,car4,Model G,43,50,77.71472888888889,0.12,61.333333333333336,Node 1,moving,1.3333333333333333,98.999,88.66666666666666,rainy,heavy,flat,road_closure,0
2023-08-30 02:18:41,car4,Model G,53,50,77.63364434104074,0.12,62.050000000000004,Node 1,moving,2.05,98.99799999999999,87.94999999999999,rainy,heavy,uphill,none,0
2023-08-30 02:19:42,car4,Model G,52,50,77.5096342960111,0.12,62.93333333333334,Node 1,moving,2.933333333333333,98.99699999999999,87.06666666666666,snowy,moderate,flat,none,0
2023-08-30 02:20:42,car4,Model G,32,50,77.39032487085555,0.12,63.800000000000004,Node 1,moving,3.8,98.99599999999998,86.19999999999999,rainy,heavy,uphill,accident_ahead,0
2023-08-30 02:17:45,car8,Model L,54,40,60.58178518518518,0.14,11.333333333333334,Node 1,moving,1.3333333333333333,97.999,138.66666666666666,snowy,light,uphill,accident_ahead,0
2023-08-30 02:21:42,car4,Model G,33,50,77.34596701035184,0.12,64.33333333333334,Node 1,moving,4.333333333333333,98.99499999999998,85.66666666666666,sunny,heavy,downhill,road_closure,0
2023-08-30 02:18:45,car8,Model L,45,40,60.39271224918518,0.14,12.233333333333334,Node 1,moving,2.2333333333333334,97.99799999999999,137.76666666666665,sunny,light,uphill,none,0
2023-08-30 02:22:42,car4,Model G,34,50,77.29872413518518,0.12,64.88333333333334,Node 1,moving,4.883333333333333,98.99399999999997,85.11666666666666,rainy,moderate,flat,accident_ahead,0
2023-08-30 02:19:45,car8,Model L,43,40,60.26204220543518,0.14,12.983333333333334,Node 1,moving,2.9833333333333334,97.99699999999999,137.01666666666665,snowy,light,downhill,none,0
2023-08-30 02:23:42,car4,Model G,31,50,77.24850539171851,0.12,65.45,Node 1,moving,5.449999999999999,98.99299999999997,84.55,snowy,heavy,downhill,none,0
2023-08-30 02:28:59,car1,Model D,94,50,68.85956052142222,0.13,111.18333333333332,Node 1,moving,1.1833333333333333,97.99799999999999,38.81666666666668,snowy,moderate,uphill,accident_ahead,0
2023-08-30 02:20:45,car8,Model L,26,40,60.14288942660647,0.14,13.700000000000001,Node 1,moving,3.7,97.99599999999998,136.3,rainy,light,uphill,accident_ahead,0
2023-08-30 02:24:42,car4,Model G,34,50,77.20694086647036,0.12,65.96666666666667,Node 1,moving,5.966666666666666,98.99199999999996,84.03333333333333,snowy,heavy,uphill,none,0
2023-08-30 02:29:59,car1,Model D,93,50,68.42958355315555,0.13,112.74999999999999,Node 1,moving,2.75,97.99699999999999,37.250000000000014,sunny,light,uphill,road_closure,0
2023-08-30 02:21:46,car8,Model L,17,40,60.100191766310175,0.14,14.133333333333335,Node 1,moving,4.133333333333334,97.99499999999998,135.86666666666667,sunny,heavy,uphill,none,0
2023-08-30 02:25:42,car4,Model G,22,50,77.15672162332962,0.12,66.53333333333333,Node 1,moving,6.533333333333332,98.99099999999996,83.46666666666667,snowy,heavy,uphill,accident_ahead,0
2023-08-30 02:30:59,car1,Model D,93,50,68.00877090355556,0.13,114.29999999999998,Node 1,moving,4.3,97.99599999999998,35.70000000000002,sunny,moderate,downhill,accident_ahead,0
2023-08-30 02:22:46,car8,Model L,12,40,60.082433095268506,0.14,14.416666666666668,Node 1,moving,4.416666666666667,97.99399999999997,135.58333333333334,rainy,heavy,downhill,accident_ahead,0
2023-08-30 02:26:42,car4,Model G,21,50,77.1362157274185,0.12,66.89999999999999,Node 1,moving,6.899999999999999,98.98999999999995,83.10000000000001,sunny,heavy,flat,accident_ahead,0
2023-08-30 02:31:59,car1,Model D,55,50,67.58795617075556,0.13,115.84999999999998,Node 1,moving,5.85,97.99499999999998,34.15000000000002,snowy,moderate,uphill,none,0
2023-08-30 02:17:51,car13,Model M,66,49,88.49475547619048,0.16,4.483333333333333,Node 1,moving,1.4833333333333334,88.999,145.51666666666668,sunny,heavy,uphill,road_closure,0
2023-08-30 02:23:46,car8,Model L,12,40,60.07388150793517,0.14,14.616666666666667,Node 1,moving,4.616666666666667,97.99299999999997,135.38333333333333,rainy,heavy,downhill,accident_ahead,0
2023-08-30 02:27:42,car4,Model G,17,50,77.11759550108516,0.12,67.24999999999999,Node 1,moving,7.249999999999998,98.98899999999995,82.75000000000001,sunny,heavy,flat,none,0
2023-08-30 02:32:59,car1,Model D,54,50,67.44228812075556,0.13,116.76666666666665,Node 1,moving,6.766666666666667,97.99399999999997,33.23333333333335,snowy,moderate,uphill,road_closure,0
2023-08-30 02:18:51,car13,Model M,59,49,88.21812885904762,0.16,5.583333333333334,Node 1,moving,2.5833333333333335,88.99799999999999,144.41666666666666,rainy,moderate,uphill,accident_ahead,0
2023-08-30 02:24:46,car8,Model L,19,40,60.0653298782685,0.14,14.816666666666666,Node 1,moving,4.816666666666667,97.99199999999996,135.18333333333334,snowy,moderate,uphill,accident_ahead,0
2023-08-30 02:28:42,car4,Model G,18,50,77.10561003408887,0.12,67.53333333333332,Node 1,moving,7.533333333333331,98.98799999999994,82.46666666666668,rainy,heavy,flat,none,0
2023-08-30 02:33:59,car1,Model D,25,50,67.30193435195557,0.13,117.66666666666666,Node 1,moving,7.666666666666667,97.99299999999997,32.33333333333334,snowy,light,uphill,accident_ahead,0
2023-08-30 02:19:51,car13,Model M,61,49,87.99751693802722,0.16,6.566666666666667,Node 1,moving,3.566666666666667,88.99699999999999,143.43333333333334,rainy,moderate,flat,none,0
2023-08-30 02:25:46,car8,Model L,15,40,60.042958427490724,0.14,15.133333333333333,Node 1,moving,5.133333333333334,97.99099999999996,134.86666666666667,rainy,heavy,uphill,accident_ahead,0